{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einfaches Machine Learning\n",
    "\n",
    "In diesem Notebook lernen wir die Grundlagen von **Machine Learning** (maschinellem Lernen) kennen.\n",
    "Wir fangen ganz einfach an: mit einer Geraden, die durch Punkte gelegt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 1: Lineare Regression\n",
    "\n",
    "Stell dir vor, du hast ein paar Datenpunkte und möchtest eine Gerade finden, die möglichst gut durch diese Punkte geht.\n",
    "Das nennt man **Lineare Regression**.\n",
    "\n",
    "Eine Gerade wird durch zwei Zahlen beschrieben:\n",
    "- **Steigung** (slope): Wie steil ist die Gerade?\n",
    "- **Y-Achsenabschnitt** (intercept): Wo schneidet die Gerade die Y-Achse?\n",
    "\n",
    "Die Formel lautet: $ y = \\text{slope} \\cdot x + \\text{intercept} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotheken laden\n",
    "\n",
    "Zuerst laden wir die Werkzeuge, die wir brauchen. Führe diese Zelle einmal aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Damit die Plots schön interaktiv sind\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsere Datenpunkte\n",
    "\n",
    "Wir erzeugen vier einfache Datenpunkte, die ungefähr auf einer Geraden liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsere vier Datenpunkte\n",
    "X = np.array([1, 2, 3, 4])       # X-Koordinaten\n",
    "Y = np.array([2.4, 3.6, 6.6, 7.4])  # Y-Koordinaten (ungefähr y = 2*x, mit Streuung)\n",
    "\n",
    "print(\"Unsere Datenpunkte:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"  Punkt {i+1}: x = {X[i]}, y = {Y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was ist der \"Loss\"?\n",
    "\n",
    "Wie gut passt unsere Gerade zu den Punkten? Das messen wir mit dem **Loss** (Verlust).\n",
    "\n",
    "Für jeden Punkt berechnen wir:\n",
    "1. Wo liegt der Punkt laut unserer Gerade? (Vorhersage)\n",
    "2. Wo liegt er wirklich? (Wahrer Wert)\n",
    "3. Wie groß ist der Unterschied? (Fehler)\n",
    "\n",
    "Der **Mean Squared Error (MSE)** ist der Durchschnitt der quadrierten Fehler:\n",
    "\n",
    "$ \\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "Je kleiner der Loss, desto besser passt die Gerade!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def berechne_loss(slope, intercept, X, Y):\n",
    "    \"\"\"Berechnet den Mean Squared Error für gegebene Parameter.\"\"\"\n",
    "    vorhersage = slope * X + intercept\n",
    "    fehler = Y - vorhersage\n",
    "    mse = np.mean(fehler ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Loss-Tal berechnen\n",
    "\n",
    "Wir berechnen jetzt den Loss für viele verschiedene Kombinationen von slope und intercept.\n",
    "Das ergibt eine 3D-Landschaft - das sogenannte **Loss-Tal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wertebereiche für slope und intercept\n",
    "slope_range = np.linspace(0, 4, 50)\n",
    "intercept_range = np.linspace(-2, 4, 50)\n",
    "\n",
    "# Gitter erstellen\n",
    "SLOPE, INTERCEPT = np.meshgrid(slope_range, intercept_range)\n",
    "\n",
    "# Loss für jeden Punkt im Gitter berechnen\n",
    "LOSS = np.zeros_like(SLOPE)\n",
    "for i in range(SLOPE.shape[0]):\n",
    "    for j in range(SLOPE.shape[1]):\n",
    "        LOSS[i, j] = berechne_loss(SLOPE[i, j], INTERCEPT[i, j], X, Y)\n",
    "\n",
    "print(\"Loss-Tal berechnet!\")\n",
    "print(f\"Minimaler Loss: {LOSS.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaktive Visualisierung\n",
    "\n",
    "Jetzt kommt das Spannende! Mit den Schiebereglern kannst du slope und intercept verändern.\n",
    "\n",
    "- **Links**: Die Datenpunkte und deine aktuelle Gerade\n",
    "- **Rechts**: Das Loss-Tal mit deiner aktuellen Position (roter Punkt)\n",
    "\n",
    "**Ziel**: Finde die Werte, bei denen der rote Punkt ganz unten im Tal liegt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure erstellen\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Linker Plot: Datenpunkte und Gerade\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.set_xlim(0, 5)\n",
    "ax1.set_ylim(-2, 12)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Datenpunkte und Gerade')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Datenpunkte zeichnen (bleiben fest)\n",
    "ax1.scatter(X, Y, color='blue', s=100, zorder=5, label='Datenpunkte')\n",
    "\n",
    "# Gerade (wird aktualisiert)\n",
    "x_line = np.linspace(0, 5, 100)\n",
    "line, = ax1.plot(x_line, x_line, 'r-', linewidth=2, label='Gerade')\n",
    "ax1.legend()\n",
    "\n",
    "# Rechter Plot: Loss-Tal (3D)\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "surface = ax2.plot_surface(SLOPE, INTERCEPT, LOSS, cmap='coolwarm', alpha=0.8, edgecolor='none')\n",
    "ax2.set_xlabel('Slope')\n",
    "ax2.set_ylabel('Intercept')\n",
    "ax2.set_zlabel('Loss')\n",
    "ax2.set_title('Loss-Tal')\n",
    "\n",
    "# Punkt-Projektion auf den Boden (Schatten) - SCHWARZ, zuerst zeichnen\n",
    "bottom_point, = ax2.plot([2], [0], [0], 'ko', markersize=8, alpha=0.4)\n",
    "\n",
    "# Verbindungslinie vom Schatten zum Punkt (gestrichelt)\n",
    "drop_line, = ax2.plot([2, 2], [0, 0], [0, berechne_loss(2, 0, X, Y)], 'k--', linewidth=0.8, alpha=0.3)\n",
    "\n",
    "# Aktueller Punkt im Loss-Tal (3D) - ROT, zuletzt zeichnen\n",
    "current_point, = ax2.plot([2], [0], [berechne_loss(2, 0, X, Y)], 'ro', markersize=10, zorder=10)\n",
    "\n",
    "# Loss-Anzeige als Text\n",
    "loss_text = fig.text(0.5, 0.02, '', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "def update(slope, intercept):\n",
    "    \"\"\"Aktualisiert die Plots wenn Schieberegler bewegt werden.\"\"\"\n",
    "    # Gerade aktualisieren\n",
    "    y_line = slope * x_line + intercept\n",
    "    line.set_ydata(y_line)\n",
    "    \n",
    "    # Aktuellen Loss berechnen\n",
    "    current_loss = berechne_loss(slope, intercept, X, Y)\n",
    "    \n",
    "    # Punkt im Loss-Tal aktualisieren (rot, 3D)\n",
    "    current_point.set_data_3d([slope], [intercept], [current_loss])\n",
    "    \n",
    "    # Verbindungslinie aktualisieren\n",
    "    drop_line.set_data_3d([slope, slope], [intercept, intercept], [0, current_loss])\n",
    "    \n",
    "    # Projektion auf Boden aktualisieren (schwarz, 2D)\n",
    "    bottom_point.set_data_3d([slope], [intercept], [0])\n",
    "    \n",
    "    # Loss-Text aktualisieren\n",
    "    loss_text.set_text(f'Aktueller Loss: {current_loss:.4f}  |  slope = {slope:.2f}, intercept = {intercept:.2f}')\n",
    "    \n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "# Schieberegler erstellen\n",
    "slope_slider = FloatSlider(\n",
    "    value=2.0,\n",
    "    min=0.0,\n",
    "    max=4.0,\n",
    "    step=0.1,\n",
    "    description='Slope:',\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "intercept_slider = FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-2.0,\n",
    "    max=4.0,\n",
    "    step=0.1,\n",
    "    description='Intercept:',\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "# Interaktive Widgets anzeigen\n",
    "interact(update, slope=slope_slider, intercept=intercept_slider)\n",
    "\n",
    "# Initiale Darstellung\n",
    "update(2.0, 0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe: Finde das Minimum!\n",
    "\n",
    "Experimentiere mit den Schiebereglern und versuche, den kleinstmöglichen Loss zu finden.\n",
    "\n",
    "1. Bei welchen Werten für **slope** und **intercept** ist der Loss am kleinsten?\n",
    "2. Wie klein ist der minimale Loss, den du erreichen kannst?\n",
    "3. Warum ist der Loss nicht genau 0? (Tipp: Schau dir die Datenpunkte an!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deine Antworten:**\n",
    "\n",
    "(Hier doppelklicken zum Bearbeiten)\n",
    "\n",
    "- Bester slope: \n",
    "- Bester intercept: \n",
    "- Minimaler Loss: \n",
    "- Warum nicht 0?: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Die mathematisch optimale Lösung\n",
    "\n",
    "Es gibt eine Formel, die direkt die besten Werte berechnet. Das nennt man die **Normalengleichung**.\n",
    "Vergleiche dein Ergebnis mit der mathematischen Lösung!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimale Lösung berechnen\n",
    "n = len(X)\n",
    "slope_optimal = (n * np.sum(X * Y) - np.sum(X) * np.sum(Y)) / (n * np.sum(X**2) - np.sum(X)**2)\n",
    "intercept_optimal = (np.sum(Y) - slope_optimal * np.sum(X)) / n\n",
    "loss_optimal = berechne_loss(slope_optimal, intercept_optimal, X, Y)\n",
    "\n",
    "print(f\"Optimale Lösung:\")\n",
    "print(f\"  slope     = {slope_optimal:.4f}\")\n",
    "print(f\"  intercept = {intercept_optimal:.4f}\")\n",
    "print(f\"  Loss      = {loss_optimal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 2: Logistische Regression (Klassifikation)\n",
    "\n",
    "Bei der linearen Regression haben wir Zahlenwerte vorhergesagt. Aber was, wenn wir entscheiden wollen, ob etwas zu **Kategorie 0** oder **Kategorie 1** gehört?\n",
    "\n",
    "Das nennt man **Klassifikation**. Zum Beispiel:\n",
    "- Ist diese E-Mail Spam (1) oder nicht (0)?\n",
    "- Besteht der Schüler die Prüfung (1) oder nicht (0)?\n",
    "\n",
    "Dafür nutzen wir die **logistische Regression**. Sie gibt uns eine Wahrscheinlichkeit zwischen 0 und 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Sigmoid-Funktion\n",
    "\n",
    "Das Herzstück ist die **Sigmoid-Funktion**. Sie verwandelt jeden Wert in eine Zahl zwischen 0 und 1:\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "Wir berechnen $z$ aus dem Abstand zum **Mittelpunkt** und der **Steilheit** (wie schnell der Übergang von 0 zu 1 passiert):\n",
    "\n",
    "$ z = \\text{steilheit} \\cdot (x - \\text{mittelpunkt}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsere Klassifikations-Daten\n",
    "\n",
    "Wir erzeugen 20 zufällige Datenpunkte. Die Klasse wird mit einer Sigmoid-Funktion (Mitte bei 3, Steilheit 1) zufällig bestimmt - je weiter rechts ein Punkt liegt, desto wahrscheinlicher gehört er zu Klasse 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Die Sigmoid-Funktion.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Zufallsgenerator mit festem Seed (für Reproduzierbarkeit)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 25 zufällige X-Werte zwischen 0 und 6\n",
    "X_klass = np.random.uniform(0, 6, 25)\n",
    "X_klass = np.sort(X_klass)  # Sortieren für bessere Übersicht\n",
    "\n",
    "# Wahrscheinlichkeit für Klasse 1 mit Sigmoid (Mitte=3, Steilheit=1)\n",
    "true_mitte = 3.0\n",
    "true_steilheit = 3.0\n",
    "p_klasse1 = sigmoid(true_steilheit * (X_klass - true_mitte))\n",
    "\n",
    "# Labels zufällig samplen basierend auf der Wahrscheinlichkeit\n",
    "Y_klass = (np.random.random(25) < p_klasse1).astype(int)\n",
    "\n",
    "print(\"Unsere Klassifikations-Daten (25 Punkte):\")\n",
    "print(f\"  Generiert mit: Mitte = {true_mitte}, Steilheit = {true_steilheit}\")\n",
    "print(f\"\\n  Links von x=3:  {sum(Y_klass[X_klass < 3])} von {sum(X_klass < 3)} sind Klasse 1\")\n",
    "print(f\"  Rechts von x=3: {sum(Y_klass[X_klass > 3])} von {sum(X_klass > 3)} sind Klasse 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der Loss: Negative Log-Likelihood\n",
    "\n",
    "Bei Klassifikation messen wir den Fehler anders. Wir nutzen die **Negative Log-Likelihood** (auch \"Binary Cross-Entropy\" genannt):\n",
    "\n",
    "$ \\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\cdot \\log(p_i) + (1-y_i) \\cdot \\log(1-p_i) \\right] $\n",
    "\n",
    "Dabei ist $p_i$ die vorhergesagte Wahrscheinlichkeit für Klasse 1.\n",
    "\n",
    "**Intuition:** \n",
    "- Wenn das Label 1 ist und wir $p = 0.99$ vorhersagen → kleiner Loss (gut!)\n",
    "- Wenn das Label 1 ist und wir $p = 0.01$ vorhersagen → großer Loss (schlecht!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def berechne_log_loss(mittelpunkt, steilheit, X, Y):\n",
    "    \"\"\"Berechnet die Negative Log-Likelihood.\"\"\"\n",
    "    z = steilheit * (X - mittelpunkt)\n",
    "    p = sigmoid(z)\n",
    "    # Kleine Epsilon-Werte um log(0) zu vermeiden\n",
    "    eps = 1e-10\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    log_loss = -np.mean(Y * np.log(p) + (1 - Y) * np.log(1 - p))\n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Loss-Gebirge berechnen\n",
    "\n",
    "Genau wie vorher berechnen wir den Loss für viele Kombinationen von Mittelpunkt und Steilheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wertebereiche für Mittelpunkt und Steilheit\n",
    "mittelpunkt_range = np.linspace(0, 6, 50)\n",
    "steilheit_range = np.linspace(0.5, 10, 50)\n",
    "\n",
    "# Gitter erstellen\n",
    "MITTEL, STEIL = np.meshgrid(mittelpunkt_range, steilheit_range)\n",
    "\n",
    "# Loss für jeden Punkt im Gitter berechnen\n",
    "LOSS_KLASS = np.zeros_like(MITTEL)\n",
    "for i in range(MITTEL.shape[0]):\n",
    "    for j in range(MITTEL.shape[1]):\n",
    "        LOSS_KLASS[i, j] = berechne_log_loss(MITTEL[i, j], STEIL[i, j], X_klass, Y_klass)\n",
    "\n",
    "print(\"Loss-Gebirge berechnet!\")\n",
    "print(f\"Minimaler Loss: {LOSS_KLASS.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaktive Visualisierung\n",
    "\n",
    "Mit den Schiebereglern kannst du **Mittelpunkt** und **Steilheit** der Sigmoid-Kurve verändern.\n",
    "\n",
    "- **Links**: Die Datenpunkte (blau = Klasse 0, orange = Klasse 1) und deine Sigmoid-Kurve\n",
    "- **Rechts**: Das Loss-Gebirge mit deiner aktuellen Position\n",
    "\n",
    "**Ziel**: Finde die Werte, bei denen der rote Punkt möglichst tief im Tal liegt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure erstellen\n",
    "fig2 = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Linker Plot: Datenpunkte und Sigmoid-Kurve\n",
    "ax3 = fig2.add_subplot(1, 2, 1)\n",
    "ax3.set_xlim(-0.5, 6.5)\n",
    "ax3.set_ylim(-0.1, 1.1)\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('Wahrscheinlichkeit für Klasse 1')\n",
    "ax3.set_title('Datenpunkte und Sigmoid-Kurve')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Datenpunkte zeichnen (Farbe nach Label)\n",
    "farben = ['blue' if y == 0 else 'orange' for y in Y_klass]\n",
    "ax3.scatter(X_klass, Y_klass, c=farben, s=100, zorder=5, edgecolors='black')\n",
    "\n",
    "# Legende manuell erstellen\n",
    "ax3.scatter([], [], c='blue', s=100, edgecolors='black', label='Klasse 0')\n",
    "ax3.scatter([], [], c='orange', s=100, edgecolors='black', label='Klasse 1')\n",
    "ax3.legend()\n",
    "\n",
    "# Sigmoid-Kurve (wird aktualisiert)\n",
    "x_sigmoid = np.linspace(-0.5, 6.5, 200)\n",
    "sigmoid_line, = ax3.plot(x_sigmoid, sigmoid(x_sigmoid), 'r-', linewidth=2)\n",
    "\n",
    "# Vertikale Linie am Mittelpunkt\n",
    "mittel_linie = ax3.axvline(x=3.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Rechter Plot: Loss-Gebirge (3D)\n",
    "ax4 = fig2.add_subplot(1, 2, 2, projection='3d')\n",
    "surface2 = ax4.plot_surface(MITTEL, STEIL, LOSS_KLASS, cmap='coolwarm', alpha=0.8, edgecolor='none')\n",
    "ax4.set_xlabel('Mittelpunkt')\n",
    "ax4.set_ylabel('Steilheit')\n",
    "ax4.set_zlabel('Loss')\n",
    "ax4.set_title('Loss-Gebirge')\n",
    "\n",
    "# Z-Boden für den Schatten: unteres Ende der Z-Achse\n",
    "z_boden = 0\n",
    "\n",
    "# Punkt-Projektion auf den Boden (Schatten) - SCHWARZ, zuerst zeichnen\n",
    "bottom_point2, = ax4.plot([3.0], [1.0], [z_boden], 'ko', markersize=8, alpha=0.4)\n",
    "\n",
    "# Verbindungslinie vom Schatten zum Punkt (gestrichelt)\n",
    "init_loss = berechne_log_loss(3.0, 1.0, X_klass, Y_klass)\n",
    "drop_line2, = ax4.plot([3.0, 3.0], [1.0, 1.0], [z_boden, init_loss], 'k--', linewidth=0.8, alpha=0.3)\n",
    "\n",
    "# Aktueller Punkt im Loss-Gebirge (3D) - ROT, zuletzt zeichnen\n",
    "current_point2, = ax4.plot([3.0], [1.0], [init_loss], 'ro', markersize=10, zorder=10)\n",
    "\n",
    "# Loss-Anzeige als Text\n",
    "loss_text2 = fig2.text(0.5, 0.02, '', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "def update_klass(mittelpunkt, steilheit):\n",
    "    \"\"\"Aktualisiert die Plots wenn Schieberegler bewegt werden.\"\"\"\n",
    "    # Sigmoid-Kurve aktualisieren\n",
    "    z = steilheit * (x_sigmoid - mittelpunkt)\n",
    "    sigmoid_line.set_ydata(sigmoid(z))\n",
    "    \n",
    "    # Mittelpunkt-Linie aktualisieren\n",
    "    mittel_linie.set_xdata([mittelpunkt, mittelpunkt])\n",
    "    \n",
    "    # Aktuellen Loss berechnen\n",
    "    current_loss = berechne_log_loss(mittelpunkt, steilheit, X_klass, Y_klass)\n",
    "    \n",
    "    # Punkt im Loss-Gebirge aktualisieren (rot, 3D)\n",
    "    current_point2.set_data_3d([mittelpunkt], [steilheit], [current_loss])\n",
    "    \n",
    "    # Verbindungslinie aktualisieren\n",
    "    drop_line2.set_data_3d([mittelpunkt, mittelpunkt], [steilheit, steilheit], [z_boden, current_loss])\n",
    "    \n",
    "    # Projektion auf Boden aktualisieren (schwarz, 2D)\n",
    "    bottom_point2.set_data_3d([mittelpunkt], [steilheit], [z_boden])\n",
    "    \n",
    "    # Loss-Text aktualisieren\n",
    "    loss_text2.set_text(f'Aktueller Loss: {current_loss:.4f}  |  Mittelpunkt = {mittelpunkt:.2f}, Steilheit = {steilheit:.2f}')\n",
    "    \n",
    "    fig2.canvas.draw_idle()\n",
    "\n",
    "# Schieberegler erstellen\n",
    "mittelpunkt_slider = FloatSlider(\n",
    "    value=3.0,\n",
    "    min=0.0,\n",
    "    max=6.0,\n",
    "    step=0.1,\n",
    "    description='Mittelpunkt:',\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "steilheit_slider = FloatSlider(\n",
    "    value=1.0,\n",
    "    min=0.5,\n",
    "    max=10.0,\n",
    "    step=0.1,\n",
    "    description='Steilheit:',\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "# Interaktive Widgets anzeigen\n",
    "interact(update_klass, mittelpunkt=mittelpunkt_slider, steilheit=steilheit_slider)\n",
    "\n",
    "# Initiale Darstellung\n",
    "update_klass(3.0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe: Finde die beste Trennung!\n",
    "\n",
    "Experimentiere mit den Schiebereglern:\n",
    "\n",
    "1. Bei welchem **Mittelpunkt** liegt die Grenze zwischen den Klassen am besten?\n",
    "2. Wie wirkt sich die **Steilheit** auf die Vorhersage aus?\n",
    "3. Was passiert mit dem Loss, wenn die Sigmoid-Kurve komplett falsch liegt (z.B. Mittelpunkt = 0)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deine Antworten:**\n",
    "\n",
    "(Hier doppelklicken zum Bearbeiten)\n",
    "\n",
    "- Bester Mittelpunkt: \n",
    "- Beste Steilheit: \n",
    "- Minimaler Loss: \n",
    "- Was passiert bei falschem Mittelpunkt?: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 3: Das gleiche mit scikit-learn\n",
    "\n",
    "Bisher haben wir alles von Hand programmiert: Loss-Funktionen, Optimierung, Plots. Das ist super, um zu verstehen, was im Hintergrund passiert!\n",
    "\n",
    "In der Praxis nutzt man aber fast immer eine **Bibliothek**, die das alles für uns erledigt. Die bekannteste in Python heißt **scikit-learn** (sprich: \"sei-kit lörn\").\n",
    "\n",
    "### Das Grundprinzip: `.fit()` und `.predict()`\n",
    "\n",
    "Scikit-learn funktioniert immer nach dem gleichen Muster -- egal ob lineare Regression, logistische Regression, Entscheidungsbäume, neuronale Netze oder hundert andere Verfahren:\n",
    "\n",
    "```python\n",
    "# 1. Modell erstellen\n",
    "modell = IrgendeinModell()\n",
    "\n",
    "# 2. Modell an Daten anpassen (= \"trainieren\")\n",
    "modell.fit(X_train, Y_train)\n",
    "\n",
    "# 3. Vorhersagen machen\n",
    "vorhersagen = modell.predict(X_neu)\n",
    "```\n",
    "\n",
    "**Das war's.** Drei Zeilen statt vieler Dutzend.\n",
    "\n",
    "- **`.fit()`** macht das, was wir oben von Hand gemacht haben: Es sucht die besten Parameter (slope, intercept, Mittelpunkt, Steilheit...), sodass der Loss minimal wird.\n",
    "- **`.predict()`** benutzt die gefundenen Parameter, um Vorhersagen für neue Daten zu machen.\n",
    "\n",
    "Das Schöne: Wenn du dieses Muster einmal verstanden hast, kannst du fast jedes ML-Verfahren in scikit-learn benutzen. Die API ist immer gleich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineare Regression mit scikit-learn\n",
    "\n",
    "Erinnere dich: Oben haben wir die optimale Gerade von Hand berechnet (Normalengleichung) und sind auf slope = 1.8 und intercept = 0.5 gekommen. Schauen wir mal, ob scikit-learn das gleiche findet.\n",
    "\n",
    "**Wichtig:** Scikit-learn erwartet die X-Daten als 2D-Array (Tabelle), nicht als einfache Liste. Das liegt daran, dass man in der Praxis oft mehrere Features (Spalten) hat. Wir haben nur ein Feature, also machen wir aus unserem 1D-Array ein 2D-Array mit einer Spalte -- das erledigt `.reshape(-1, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Schritt 1: Modell erstellen\n",
    "lin_modell = LinearRegression()\n",
    "\n",
    "# Schritt 2: Modell trainieren (fit)\n",
    "# reshape(-1, 1) macht aus [1, 2, 3, 4] eine \"Tabelle\" mit einer Spalte:\n",
    "#   [[1],\n",
    "#    [2],\n",
    "#    [3],\n",
    "#    [4]]\n",
    "lin_modell.fit(X.reshape(-1, 1), Y)\n",
    "\n",
    "# Was hat scikit-learn gefunden?\n",
    "print(\"Scikit-learn hat folgende Parameter gefunden:\")\n",
    "print(f\"  slope     = {lin_modell.coef_[0]:.4f}\")\n",
    "print(f\"  intercept = {lin_modell.intercept_:.4f}\")\n",
    "print()\n",
    "print(\"Vergleich mit unserer Normalengleichung:\")\n",
    "print(f\"  slope     = {slope_optimal:.4f}  (identisch!)\")\n",
    "print(f\"  intercept = {intercept_optimal:.4f}  (identisch!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exakt die gleichen Werte! Scikit-learn hat im Hintergrund genau das gemacht, was wir vorher von Hand berechnet haben.\n",
    "\n",
    "Jetzt können wir mit `.predict()` Vorhersagen machen -- auch für x-Werte, die gar nicht in unseren Daten vorkommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 3: Vorhersagen machen (predict)\n",
    "x_neu = np.array([0, 1.5, 2.5, 5, 10]).reshape(-1, 1)\n",
    "vorhersagen = lin_modell.predict(x_neu)\n",
    "\n",
    "print(\"Vorhersagen für neue x-Werte:\")\n",
    "for xi, yi in zip(x_neu.flatten(), vorhersagen):\n",
    "    print(f\"  x = {xi:5.1f}  -->  y = {yi:.2f}\")\n",
    "\n",
    "# Visualisierung\n",
    "fig3, ax5 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Originale Datenpunkte\n",
    "ax5.scatter(X, Y, color='blue', s=100, zorder=5, label='Trainingsdaten')\n",
    "\n",
    "# Gerade von scikit-learn\n",
    "x_plot = np.linspace(0, 5, 100).reshape(-1, 1)\n",
    "y_plot = lin_modell.predict(x_plot)\n",
    "ax5.plot(x_plot, y_plot, 'r-', linewidth=2, label='scikit-learn Gerade')\n",
    "\n",
    "# Neue Vorhersagen hervorheben\n",
    "ax5.scatter(x_neu[:-1], vorhersagen[:-1], color='green', s=120, zorder=5,\n",
    "            marker='*', label='Vorhersagen')\n",
    "\n",
    "ax5.set_xlabel('x')\n",
    "ax5.set_ylabel('y')\n",
    "ax5.set_title('Lineare Regression mit scikit-learn')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistische Regression mit scikit-learn\n",
    "\n",
    "Jetzt das gleiche Spiel für die Klassifikation. Wir benutzen die gleichen Daten (`X_klass`, `Y_klass`) von oben und lassen scikit-learn die beste Sigmoid-Kurve finden.\n",
    "\n",
    "Der Code sieht fast identisch aus -- nur statt `LinearRegression` nehmen wir `LogisticRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Schritt 1: Modell erstellen\n",
    "# penalty=None schaltet die \"Regularisierung\" aus -- damit findet scikit-learn\n",
    "# genau die gleiche Lösung, die wir oben mit den Schiebereglern gesucht haben.\n",
    "# (Regularisierung ist ein fortgeschrittenes Thema für später.)\n",
    "log_modell = LogisticRegression(penalty=None)\n",
    "\n",
    "# Schritt 2: Modell trainieren (fit) -- gleiche Daten wie oben!\n",
    "log_modell.fit(X_klass.reshape(-1, 1), Y_klass)\n",
    "\n",
    "# Die Parameter, die scikit-learn gefunden hat\n",
    "sk_steilheit = log_modell.coef_[0][0]\n",
    "sk_mittelpunkt = -log_modell.intercept_[0] / sk_steilheit\n",
    "\n",
    "print(\"Scikit-learn hat folgende Parameter gefunden:\")\n",
    "print(f\"  Steilheit   = {sk_steilheit:.4f}\")\n",
    "print(f\"  Mittelpunkt = {sk_mittelpunkt:.4f}\")\n",
    "print()\n",
    "print(f\"Zur Erinnerung: Die Daten wurden mit Mitte={true_mitte}, Steilheit={true_steilheit} generiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt nutzen wir `.predict()` um Klassen vorherzusagen, und `.predict_proba()` um die Wahrscheinlichkeiten zu sehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 3: Vorhersagen machen\n",
    "x_test = np.array([1.0, 2.5, 3.0, 3.5, 5.0]).reshape(-1, 1)\n",
    "\n",
    "# .predict() gibt die Klasse (0 oder 1)\n",
    "klassen = log_modell.predict(x_test)\n",
    "\n",
    "# .predict_proba() gibt die Wahrscheinlichkeiten für jede Klasse\n",
    "wahrscheinlichkeiten = log_modell.predict_proba(x_test)\n",
    "\n",
    "print(\"Vorhersagen für neue x-Werte:\")\n",
    "print(f\"  {'x':>5}  {'Klasse':>6}  {'P(Klasse 0)':>12}  {'P(Klasse 1)':>12}\")\n",
    "print(f\"  {'─'*5}  {'─'*6}  {'─'*12}  {'─'*12}\")\n",
    "for xi, ki, wi in zip(x_test.flatten(), klassen, wahrscheinlichkeiten):\n",
    "    print(f\"  {xi:5.1f}  {ki:6d}  {wi[0]:12.1%}  {wi[1]:12.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung: Datenpunkte + scikit-learn Sigmoid-Kurve\n",
    "fig4, ax6 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Datenpunkte (Farbe nach Label)\n",
    "farben = ['blue' if y == 0 else 'orange' for y in Y_klass]\n",
    "ax6.scatter(X_klass, Y_klass, c=farben, s=100, zorder=5, edgecolors='black')\n",
    "\n",
    "# Legende\n",
    "ax6.scatter([], [], c='blue', s=100, edgecolors='black', label='Klasse 0')\n",
    "ax6.scatter([], [], c='orange', s=100, edgecolors='black', label='Klasse 1')\n",
    "\n",
    "# Sigmoid-Kurve von scikit-learn\n",
    "x_plot_sig = np.linspace(-0.5, 6.5, 200).reshape(-1, 1)\n",
    "p_plot = log_modell.predict_proba(x_plot_sig)[:, 1]  # Wahrscheinlichkeit für Klasse 1\n",
    "ax6.plot(x_plot_sig, p_plot, 'r-', linewidth=2, label='scikit-learn Sigmoid')\n",
    "\n",
    "# Entscheidungsgrenze (wo P = 0.5)\n",
    "ax6.axvline(x=sk_mittelpunkt, color='gray', linestyle='--', alpha=0.5,\n",
    "            label=f'Grenze bei x={sk_mittelpunkt:.2f}')\n",
    "ax6.axhline(y=0.5, color='gray', linestyle=':', alpha=0.3)\n",
    "\n",
    "ax6.set_xlabel('x')\n",
    "ax6.set_ylabel('Wahrscheinlichkeit für Klasse 1')\n",
    "ax6.set_title('Logistische Regression mit scikit-learn')\n",
    "ax6.legend(loc='center right')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenfassung\n",
    "\n",
    "Was wir gelernt haben:\n",
    "\n",
    "| | Von Hand | Mit scikit-learn |\n",
    "|---|---|---|\n",
    "| **Lineare Regression** | Normalengleichung, ~10 Zeilen | `LinearRegression()` + `.fit()` + `.predict()` |\n",
    "| **Logistische Regression** | Loss-Funktion + Schieberegler, ~30 Zeilen | `LogisticRegression()` + `.fit()` + `.predict()` |\n",
    "\n",
    "Das Muster ist immer gleich:\n",
    "1. **Modell erstellen** -- z.B. `LinearRegression()` oder `LogisticRegression()`\n",
    "2. **`.fit(X, Y)`** -- das Modell lernt aus den Daten\n",
    "3. **`.predict(X_neu)`** -- das Modell macht Vorhersagen\n",
    "\n",
    "Dieses Prinzip funktioniert genauso für Entscheidungsbäume, Random Forests, Support Vector Machines und viele weitere Verfahren in scikit-learn. Wenn du `.fit()` und `.predict()` verstanden hast, kannst du sie alle benutzen!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
